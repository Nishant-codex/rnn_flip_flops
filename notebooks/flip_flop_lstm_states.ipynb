{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "flip_flop_lstm_states",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNVW3WTzjH9nZAep1UGQqCi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nishant-codex/rnn_flip_flops/blob/master/flip_flop_lstm_states.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBoytjuF6HpE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "# import tensorflow as tf\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import cProfile\n",
        "# %tensorflow_version 1.x magic\n",
        "import matplotlib.pyplot as plt\n",
        "np.random.seed(400)\n",
        "# if(tf.enable_eager_execution()):\n",
        "#   print('true')\n",
        "class FlipFlop:\n",
        "#Hyperparameters\n",
        "    \n",
        "    hyp_dict = \\\n",
        "    {'time' : 500,\n",
        "    'bits' : 3 ,\n",
        "    'num_steps' : 6,\n",
        "    'batch_size' : 200,\n",
        "    'state_size' : 50 ,\n",
        "    'num_classes' : 2,\n",
        "    'p': 0.95,\n",
        "    'learning_rate' :0.01,\n",
        "    'c_type':'GRU'\n",
        "    }\n",
        "    '''\n",
        "    Architectures: \n",
        "    Vanilla, UG-RNN, GRU, LSTM\n",
        "    \n",
        "    Activation: \n",
        "    Tanh, relu\n",
        "    \n",
        "    Num_units:\n",
        "    64, 128, 256 \n",
        "    \n",
        "    L2 regularization: \n",
        "    1e-5, 1e-4, 1e-3, 1e-2\n",
        "    '''\n",
        "    def __init__(self,\n",
        "        time = hyp_dict['time'],\n",
        "        bits = hyp_dict['bits'],\n",
        "        num_steps = hyp_dict['num_steps'],\n",
        "        batch_size = hyp_dict['batch_size'],\n",
        "        state_size = hyp_dict['state_size'],\n",
        "        num_classes = hyp_dict['num_classes'],\n",
        "        learning_rate = hyp_dict['learning_rate'],\n",
        "        p = hyp_dict['p'],\n",
        "        c_type = hyp_dict['c_type']):\n",
        "        \n",
        "        self.time = time\n",
        "        self.bits = bits\n",
        "        self.num_steps = num_steps\n",
        "        self.num_steps =num_steps \n",
        "        self.batch_size = batch_size \n",
        "        self.state_size = state_size \n",
        "        self.num_classes = num_classes\n",
        "        self.learning_rate = learning_rate\n",
        "        self.p = p  \n",
        "        self.c_type = c_type    \n",
        "        self.sess = 0\n",
        "        self.test_data = []\n",
        "\n",
        "    def flip_flop(self, plot=False):\n",
        "    \n",
        "      inputs = []\n",
        "      outputs= []\n",
        "      for batch in range(self.batch_size):\n",
        "        a = np.random.binomial(1,self.p,size=[self.bits, self.time])\n",
        "        b = np.random.binomial(1,self.p,size=[self.bits, self.time])\n",
        "        inp_= a-b\n",
        "        last = 1\n",
        "        out_ = np.ones_like(inp_)\n",
        "        for i in range(self.bits):\n",
        "          for m in range(self.time):\n",
        "            a = inp_[i,m]\n",
        "            if a !=0:\n",
        "              last = a\n",
        "            out_[i,m] = last\n",
        "        # if(plot):\n",
        "        inp_inv = inp_.T\n",
        "        out_inv = out_.T\n",
        "        # print(inp_[:,0])\n",
        "        inputs.append(inp_inv)\n",
        "        outputs.append(out_inv)\n",
        "      if(plot):\n",
        "        plt.plot(inputs[1][:,0])\n",
        "        plt.plot(outputs[1][:,0])\n",
        "        plt.xlabel(\"time\")\n",
        "        plt.ylabel(\"Bit 0\")\n",
        "        plt.show()\n",
        "        plt.plot(inputs[1][:,1])\n",
        "        plt.plot(outputs[1][:,1])\n",
        "        plt.xlabel(\"time\")\n",
        "        plt.ylabel(\"Bit 1\")\n",
        "        plt.show()\n",
        "        plt.plot(inputs[1][:,2])\n",
        "        plt.plot(outputs[1][:,2])\n",
        "        plt.xlabel(\"time\")\n",
        "        plt.ylabel(\"Bit 2\")\n",
        "        plt.show()\n",
        "      return({'inputs':inputs,'outputs':outputs})\n",
        "    \n",
        "    def data_batches(self,num_batches):\n",
        "      batch_list = []\n",
        "      for i in range(num_batches):\n",
        "        batch_list.append(self.flip_flop())\n",
        "      return batch_list\n",
        "  \n",
        "    def reset_graph(self):\n",
        "        if 'sess' in globals() and sess:\n",
        "            sess.close()\n",
        "        tf.reset_default_graph()\n",
        "    \n",
        "    def setup_model(self):\n",
        "      \n",
        "      self.reset_graph()\n",
        "      x = tf.placeholder(tf.float32, [self.batch_size, self.time, self.bits], name='input_placeholder')\n",
        "      y = tf.placeholder(tf.float32, [self.batch_size, self.time, self.bits], name='labels_placeholder')\n",
        "    \n",
        "      if self.c_type == 'Vanilla':\n",
        "        cell = tf.contrib.rnn.BasicRNNCell(self.state_size,reuse=tf.AUTO_REUSE)\n",
        "    \n",
        "      if self.c_type == 'GRU':\n",
        "        cell = tf.contrib.rnn.GRUCell(self.state_size,reuse=tf.AUTO_REUSE)\n",
        "    \n",
        "      if self.c_type == 'LSTM':\n",
        "        cell = tf.contrib.rnn.LSTMCell(self.state_size,reuse=tf.AUTO_REUSE,state_is_tuple=True)\n",
        "    \n",
        "      init_state = cell.zero_state(self.batch_size, dtype=tf.float32)\n",
        "    \n",
        "      rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, x, initial_state=init_state,)\n",
        "    \n",
        "      \"\"\"\n",
        "      rnn_outputs gives out rnn hidden states ht which is of the size [batch_size, timestep, state_size]\n",
        "    \n",
        "      \"\"\"\n",
        "#      self.cell = cell\n",
        "      with tf.variable_scope('losses',reuse=tf.AUTO_REUSE):\n",
        "          W = tf.get_variable('W', [self.state_size , self.bits])\n",
        "          b = tf.get_variable('b', [self.bits], initializer=tf.constant_initializer(0.0))\n",
        "      # plt.plot(W)\n",
        "      rnn_outputs_ = tf.reshape(rnn_outputs, [-1, self.state_size])\n",
        "    \n",
        "      logits = tf.tensordot(rnn_outputs_,W,axes=1) + b\n",
        "      # predictions = tf.nn.softmax(logits)\n",
        "    \n",
        "      y_as_list =tf.reshape(y, [-1, self.bits]) #shape is flattened_tensor x bits\n",
        "      print(y_as_list.shape)\n",
        "      losses = tf.squared_difference(logits,y_as_list)\n",
        "      total_loss = tf.reduce_mean(losses)\n",
        "      train_step = tf.train.AdagradOptimizer(self.learning_rate).minimize(total_loss)\n",
        "      return {'losses':total_loss, 'train_step':train_step, \n",
        "              'hiddens':rnn_outputs,'finalstate':final_state , \n",
        "              'X':x, 'Y':y, 'predict':logits, 'init_state':init_state , \n",
        "              'saver' : tf.train.Saver(),'cell':cell, 'weights':W }\n",
        "    \n",
        "    \n",
        "    \n",
        "    def train_network(self, num_epochs, verbose=True, save=True):\n",
        "      num_epochs = num_epochs\n",
        "      act = self.setup_model()\n",
        "      data = self.data_batches(num_epochs)\n",
        "      epochs = 0\n",
        "      training_loss = 0\n",
        "      hidden = []\n",
        "      # path = input('What should be the name of the file?')\n",
        "      # writer = tf.summary.FileWriter('./graphs')\n",
        "      saver = tf.train.Saver()\n",
        "      path = self.c_type+str(num_epochs)\n",
        "    \n",
        "      with tf.Session() as sess:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        training_losses = []\n",
        "        for i in range(len(data)):\n",
        "          ground_truth = data[i]['outputs']\n",
        "          tr_losses, training_step_, training_state, outputs, predict = \\\n",
        "                      sess.run([act['losses'],\n",
        "                                act['train_step'],\n",
        "                                act['finalstate'],\n",
        "                                act['hiddens'],\n",
        "                                act['predict']],\n",
        "                                    feed_dict={act['X']:data[i]['inputs'], act['Y']:data[i]['outputs']})\n",
        "          training_loss += tr_losses\n",
        "    \n",
        "          if verbose:\n",
        "              print(\"Average training loss for Epoch\", epochs, \":\", training_loss/100)\n",
        "          training_losses.append(training_loss)\n",
        "          training_loss = 0\n",
        "          if epochs == 0:\n",
        "            hidden.append(outputs)\n",
        "          epochs +=1\n",
        "        hidden.append(outputs)\n",
        "        if(save):\n",
        "          saver.save(sess, path)\n",
        "      return {'losses':training_losses, 'hidden':hidden, 'predictions':predict, 'truth':ground_truth, 'training':training_state}\n",
        "    def lstm_hiddens(self, graph):\n",
        "\n",
        "      n_hidden = self.state_size\n",
        "      [self.batch_size, self.time, self.bits] = np.array(self.test_data['inputs']).shape\n",
        "      initial_state = graph['cell'].zero_state(a.batch_size, dtype=tf.float32)\n",
        "\n",
        "      ''' Add ops to the graph for getting the complete LSTM state\n",
        "      (i.e., hidden and cell) at every timestep.'''\n",
        "      full_state_list = []\n",
        "      # cur_state_min_one = 0\n",
        "      for t in range(self.time):\n",
        "          input_ = graph['X'][:,t,:]\n",
        "          if t == 0:\n",
        "              cur_state_min_one = initial_state\n",
        "          else:\n",
        "              cur_state_min_one = full_state_list[-1]\n",
        "\n",
        "          _, states = graph['cell'](input_,cur_state_min_one)\n",
        "          full_state_list.append(states)\n",
        "\n",
        "      '''Evaluate those ops'''\n",
        "      ops_to_eval = [full_state_list]\n",
        "      feed_dict = {graph['X']: data['inputs']}\n",
        "      ev_full_state_list= \\\n",
        "          sess.run(ops_to_eval, feed_dict=feed_dict)\n",
        "\n",
        "      '''Package the results'''\n",
        "      h = np.zeros([self.batch_size, self.time, self.state_size]) # hidden states: bxtxd\n",
        "      c = np.zeros([self.batch_size, self.time, self.state_size]) # cell states: bxtxd\n",
        "      for t in range(self.time):\n",
        "          h[:,t,:] = ev_full_state_list[0][t].h\n",
        "          c[:,t,:] = ev_full_state_list[0][t].c\n",
        "\n",
        "      ev_LSTMCellState = tf.nn.rnn_cell.LSTMStateTuple(h=h, c=c)\n",
        "      return(ev_LSTMCellState)\n",
        "\n",
        "\n",
        "    def reload_from_checkpoints(self, chkpt):\n",
        "      # Uncomment to run a saved network\n",
        "      # Need to close the session manually \n",
        "      graph = self.setup_model()\n",
        "      saver = graph['saver']\n",
        "      state = None\n",
        "      self.test_data = self.flip_flop()\n",
        "      feed_dict = {graph['X']:self.test_data['inputs'],graph['Y']:self.test_data['outputs']}\n",
        "      sess = tf.Session()\n",
        "      sess.run(tf.global_variables_initializer())\n",
        "      saver.restore(sess, chkpt)\n",
        "      hiddens, outputs, losses = sess.run([graph['hiddens'],graph['predict'],graph['losses']],feed_dict=feed_dict)\n",
        "      \n",
        "      if(self.c_type=='LSTM'):\n",
        "        return({'hiddens':lstm_hiddens(graph),'predictions':outputs,'loss':losses})\n",
        "\n",
        "      else:\n",
        "        return({'hiddens':hiddens,'predictions':outputs,'loss':losses})\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxxheJPkBKxF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "74779373-d46b-4a47-a7ca-d7a8b01dae03"
      },
      "source": [
        "a = FlipFlop(c_type='LSTM')\n",
        "m = a.train_network(20)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(100000, 3)\n",
            "Average training loss for Epoch 0 : 0.010032001733779907\n",
            "Average training loss for Epoch 1 : 0.010023872852325439\n",
            "Average training loss for Epoch 2 : 0.010017368793487549\n",
            "Average training loss for Epoch 3 : 0.010009739398956299\n",
            "Average training loss for Epoch 4 : 0.010003750324249267\n",
            "Average training loss for Epoch 5 : 0.009995098114013673\n",
            "Average training loss for Epoch 6 : 0.009989465475082398\n",
            "Average training loss for Epoch 7 : 0.009979455471038819\n",
            "Average training loss for Epoch 8 : 0.009972199201583862\n",
            "Average training loss for Epoch 9 : 0.009964672923088074\n",
            "Average training loss for Epoch 10 : 0.009958530068397522\n",
            "Average training loss for Epoch 11 : 0.009947331547737122\n",
            "Average training loss for Epoch 12 : 0.009944878220558166\n",
            "Average training loss for Epoch 13 : 0.009934483170509338\n",
            "Average training loss for Epoch 14 : 0.009930240511894227\n",
            "Average training loss for Epoch 15 : 0.009924468994140625\n",
            "Average training loss for Epoch 16 : 0.009915614724159241\n",
            "Average training loss for Epoch 17 : 0.009907745718955994\n",
            "Average training loss for Epoch 18 : 0.009898670911788941\n",
            "Average training loss for Epoch 19 : 0.009890166521072387\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFgGvQ6R5Qhf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrQA9V80DZzV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}